{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.agents.format_scratchpad.tools import format_to_tool_messages\n",
    "from langchain_core import prompts\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from vertexai.preview import reasoning_engines\n",
    "from langchain_google_vertexai import HarmBlockThreshold, HarmCategory\n",
    "from google.cloud import trace_v1 as trace\n",
    "from vertexai.reasoning_engines._reasoning_engines import _utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(\n",
    "    project='annular-weaver-428312-s3',\n",
    "    location='us-central1',\n",
    "    staging_bucket='gs://agent-deploy-ssp',\n",
    ")\n",
    "\n",
    "model = 'gemini-1.5-flash-001'\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "}\n",
    "\n",
    "model_kwargs = {\n",
    "    'temperature': 0.2,\n",
    "    'max_output_tokens': 8192,\n",
    "    'top_p': 0.95,\n",
    "    'top_k': 40,\n",
    "    'safety_settings': safety_settings,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_prompt = \"\"\" Você é o Assistente Virtual da Delegacia de Polícia do Estado do Piauí. \n",
    "\n",
    "Você é responsável pelo atendimento de relatos da população para o serviço de emergência 190.\\n\n",
    "\n",
    "Nesse serviço, você possui dois possíveis caminhos:\n",
    "\n",
    "    1. Atendimento emergencial de ocorrências que representem ameaça à vida ou ameaça iminente.\n",
    "    2. Atendimento de relatos da população para registro de boletins de ocorrência não emergenciais.\n",
    "\n",
    "Sua função é analisar a solicitação do cidadão por meio do fluxo de atendimento do 190, compreender a natureza da ocorrência e determinar o nível de urgência.\n",
    "\n",
    "Se a solicitação do cidadão for:\n",
    "\n",
    "## Caso Emergencial ou Ameaça Iminente:\n",
    "    - Faça perguntas para entender a situação rapidamente.\n",
    "    - Colete o mínimo de informações possível, apenas o essencial, como a localização.\n",
    "    - Informe ao cidadão que a ocorrência será repassada a um oficial superior para avaliação e possível intervenção.\n",
    "\n",
    "## Registro de Boletim de Ocorrência Não Emergencial:\n",
    "    - Extraia informações precisas do relato do cidadão.\n",
    "    - Preencha o boletim de ocorrência com os dados fornecidos.\n",
    "\n",
    "Exemplo de interação:\n",
    "\n",
    "    - Cidadão: \"Dois bandidos estão tentando invadir minha casa, me ajuda!\"\n",
    "\n",
    "    - Você: \"Entendo a gravidade da situação. Por favor, mantenha a calma e me forneça as seguintes informações para que possamos enviar ajuda imediatamente:\n",
    "\n",
    "            1. Qual é o endereço exato onde você se encontra?\n",
    "            2. Você está em um local seguro no momento?\n",
    "            3. Você pode descrever os suspeitos ou fornecer alguma informação adicional sobre eles?\n",
    "\n",
    "            Vamos agir rapidamente para garantir sua segurança.\"\n",
    "\n",
    "Sua conduta deve sempre ser atenciosa e respeitosa com o cidadão.\n",
    "\n",
    "Se o cidadão utilizar xingamentos ou palavrões, mantenha a calma, e tente redirecionar a conversa para um tom mais produtivo.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = {\n",
    "    'history': lambda x: x['history'],\n",
    "    'input': lambda x: x['input'],\n",
    "    'agent_scratchpad': (lambda x: format_to_tool_messages(x['intermediate_steps'])),\n",
    "} | prompts.ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', ai_prompt),\n",
    "        prompts.MessagesPlaceholder(variable_name='history'),\n",
    "        ('user', '{input}'),\n",
    "        prompts.MessagesPlaceholder(variable_name='agent_scratchpad'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Creating Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = reasoning_engines.LangchainAgent(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    chat_history=get_session_history,\n",
    "    agent_executor_kwargs={'return_intermediate_steps': False},\n",
    ")\n",
    "\n",
    "# agent.query(\n",
    "#    input=\"Dois bandidos estão entrando agora na minha casa.\",\n",
    "#    config={\"configurable\": {\"session_id\": \"dem\"}},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Deploying in GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bucket agent-deploy-ssp\n",
      "Writing to gs://agent-deploy-ssp/reasoning_engine/reasoning_engine.pkl\n",
      "Writing to gs://agent-deploy-ssp/reasoning_engine/requirements.txt\n",
      "Creating in-memory tarfile of extra_packages\n",
      "Writing to gs://agent-deploy-ssp/reasoning_engine/dependencies.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1720731811.245137 8877860 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache, work_serializer_dispatch\n",
      "I0000 00:00:1720731811.262319 8877860 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ReasoningEngine\n",
      "Create ReasoningEngine backing LRO: projects/370200532108/locations/us-central1/reasoningEngines/8580025793210482688/operations/738659367603142656\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "500 The user created Reasoning Engine failed to start and cannot serve traffic. 13: The user created Reasoning Engine failed to start and cannot serve traffic.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb Célula 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m remote_agent \u001b[39m=\u001b[39m reasoning_engines\u001b[39m.\u001b[39;49mReasoningEngine\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     agent,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     requirements\u001b[39m=\u001b[39;49m[\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mgoogle-cloud-aiplatform[langchain,reasoningengine]\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mcloudpickle==3.0.0\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpydantic==2.7.4\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mlangchain-google-community\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mgoogle-cloud-discoveryengine\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     ],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     display_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mAgente SSP 190\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/ufpi/bot_ssp/bot-ssp-pln/lib/python3.11/site-packages/vertexai/reasoning_engines/_reasoning_engines.py:284\u001b[0m, in \u001b[0;36mReasoningEngine.create\u001b[0;34m(cls, reasoning_engine, requirements, reasoning_engine_name, display_name, description, gcs_dir_name, sys_version, extra_packages)\u001b[0m\n\u001b[1;32m    272\u001b[0m operation_future \u001b[39m=\u001b[39m sdk_resource\u001b[39m.\u001b[39mapi_client\u001b[39m.\u001b[39mcreate_reasoning_engine(\n\u001b[1;32m    273\u001b[0m     parent\u001b[39m=\u001b[39minitializer\u001b[39m.\u001b[39mglobal_config\u001b[39m.\u001b[39mcommon_location_path(\n\u001b[1;32m    274\u001b[0m         project\u001b[39m=\u001b[39msdk_resource\u001b[39m.\u001b[39mproject, location\u001b[39m=\u001b[39msdk_resource\u001b[39m.\u001b[39mlocation\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     ),\n\u001b[1;32m    282\u001b[0m )\n\u001b[1;32m    283\u001b[0m _LOGGER\u001b[39m.\u001b[39mlog_create_with_lro(\u001b[39mcls\u001b[39m, operation_future)\n\u001b[0;32m--> 284\u001b[0m created_resource \u001b[39m=\u001b[39m operation_future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    285\u001b[0m _LOGGER\u001b[39m.\u001b[39mlog_create_complete(\n\u001b[1;32m    286\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m    287\u001b[0m     created_resource,\n\u001b[1;32m    288\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_resource_noun,\n\u001b[1;32m    289\u001b[0m     module_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvertexai.preview.reasoning_engines\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    290\u001b[0m )\n\u001b[1;32m    291\u001b[0m \u001b[39m# We use `._get_gca_resource(...)` instead of `created_resource` to\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m# fully instantiate the attributes of the reasoning engine.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ufpi/bot_ssp/bot-ssp-pln/lib/python3.11/site-packages/google/api_core/future/polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking_poll(timeout\u001b[39m=\u001b[39mtimeout, retry\u001b[39m=\u001b[39mretry, polling\u001b[39m=\u001b[39mpolling)\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[39m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    263\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "\u001b[0;31mInternalServerError\u001b[0m: 500 The user created Reasoning Engine failed to start and cannot serve traffic. 13: The user created Reasoning Engine failed to start and cannot serve traffic."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1720735418.145348 8884196 tcp_posix.cc:809] IOMGR endpoint shutdown\n"
     ]
    }
   ],
   "source": [
    "remote_agent = reasoning_engines.ReasoningEngine.create(\n",
    "    agent,\n",
    "    requirements=[\n",
    "        'google-cloud-aiplatform[langchain,reasoningengine]',\n",
    "        'cloudpickle==3.0.0',\n",
    "        'pydantic==2.7.4',\n",
    "        'langchain-google-community',\n",
    "        'google-cloud-discoveryengine',\n",
    "    ],\n",
    "    display_name='Agente SSP 190',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Testing in Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1720728981.760517 8758691 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1720728984.541871 8813507 tcp_posix.cc:809] IOMGR endpoint shutdown\n",
      "I0000 00:00:1720728984.542615 8758691 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1720728987.680488 8813511 tcp_posix.cc:809] IOMGR endpoint shutdown\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Entendo a sua situação e a sua preocupação. Por favor, mantenha a calma. Para que possamos enviar ajuda imediatamente, preciso de algumas informações:\n",
       "\n",
       "1. Qual o número da rua Alexandre?\n",
       "2. Você pode descrever o que está acontecendo? \n",
       "3. Há outras pessoas em risco?\n",
       "\n",
       "A polícia está a caminho, mas precisamos de mais detalhes para direcioná-los com precisão. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PROJECT_ID = 'annular-weaver-428312-s3'\n",
    "LOCATION = 'us-central1'\n",
    "REASONING_ENGINE_ID = '2705080079305670656'\n",
    "\n",
    "remote_agent = reasoning_engines.ReasoningEngine(\n",
    "    f'projects/{PROJECT_ID}/locations/{LOCATION}/reasoningEngines/{REASONING_ENGINE_ID}'\n",
    ")\n",
    "\n",
    "query = 'Rua alexandre e estou seguro, vem logo porra'\n",
    "response = remote_agent.query(input=query, config={'configurable': {'session_id': 'demons'}})\n",
    "display(Markdown(response['output']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1720726745.700290 8758691 work_stealing_thread_pool.cc:321] WorkStealingThreadPoolImpl::PrepareFork\n",
      "I0000 00:00:1720726746.280371 8758691 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1720726746.286643 8772740 tcp_posix.cc:809] IOMGR endpoint shutdown\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb Célula 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m client \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39mTraceServiceClient()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m result \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     r\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m client\u001b[39m.\u001b[39mlist_traces(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m ]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m trace_data \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mget_trace(project_id\u001b[39m=\u001b[39mPROJECT_ID, trace_id\u001b[39m=\u001b[39mresult[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mtrace_id)\u001b[39m.\u001b[39mspans[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luishmq/Documents/ufpi/bot_ssp/notebooks/agent_gcp.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m trace_data\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1720726989.214820 8772736 tcp_posix.cc:809] IOMGR endpoint shutdown\n"
     ]
    }
   ],
   "source": [
    "client = trace.TraceServiceClient()\n",
    "\n",
    "result = [\n",
    "    r\n",
    "    for r in client.list_traces(\n",
    "        request=trace.types.ListTracesRequest(\n",
    "            project_id=PROJECT_ID,\n",
    "            # Return all traces containing `labels {key: \"openinference.span.kind\" value: \"AGENT\"}`\n",
    "            filter='openinference.span.kind:AGENT',\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "trace_data = client.get_trace(project_id=PROJECT_ID, trace_id=result[0].trace_id)\n",
    "\n",
    "spans = pd.DataFrame.from_records([_utils.to_dict(span) for span in trace_data.spans])\n",
    "spans.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
